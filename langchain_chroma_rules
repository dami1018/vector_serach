import os
# 指定包含 PDF 文件的主目录路径
directory_path = r'D:\script\python\data\\'
# 列出目录及其子目录下所有以 .pdf 结尾的文件
pdf_files = []
for root, dirs, files in os.walk(directory_path):
    for file in files:
        if file.endswith('.pdf'):
            pdf_files.append(os.path.join(root, file))

# 打印找到的 PDF 文件
print(f"在目录 '{directory_path}' 及其子目录中找到的 PDF 文件数量: {len(pdf_files)}\n")
# 列表的遍历
from langchain.document_loaders import PyPDFLoader
# 实验,成功，系统自动获取导出的文件路径不需要再加修饰了，比如前面加r
loader = PyPDFLoader(pdf_files[2])
file_content = loader.load()
data = file_content[2]
print(file_content[2])
page_content = data.page_content
print(page_content)
page_content = page_content.replace('宋彦泽','')
page_content = page_content.replace('CSC26272','')
file_content[2].page_content = page_content
for content in file_content:
    page_content = content.page_content
# 这里的file_content就是一个list,每个元素是document类型的，有两个属性，一个是字符串，一个是字典
file_content[0]
len(file_content)
pdf = []
for pdf_file in pdf_files:
    loader = PyPDFLoader(pdf_file)
    file_content = loader.load()
    for content in file_content:
        content.page_content = content.page_content.replace('宋彦泽','')
        content.page_content = content.page_content.replace('CSC26272','')
        pdf.append(content)
print(len(pdf))
print(pdf[305])
# 建立向量数据库

from langchain.vectorstores import chroma
from langchain_community.embeddings import OllamaEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size=50,chunk_overlap=5,length_function=len,add_start_index=True)
# data = text_splitter.split_documets(document=content)
data = text_splitter.split_documents(documents=pdf) 

embeddings = OllamaEmbeddings(model="llama3:8b")
persist_path = r'D:\script\python\data\chroma_db\rules\\'
# 运行时间过长可以考虑分块再大一些，同时换一个分词器
rules_db = chroma.Chroma.from_documents(data, embeddings,persist_directory=persist_path)       

from langchain.embeddings import SentenceTransformerEmbeddings
embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=100,length_function=len,add_start_index=True)
data = text_splitter.split_documents(documents=pdf)
# data[500]
rules_db = chroma.Chroma.from_documents(data, embeddings,persist_directory=persist_path)   
import torch
print(torch.__version__)
print(torch.__version__)

#查找和搜寻
# 查找在数据库中寻找相似的向量，返回的doc是document类型
question = "客户开展融资融券业务应该具备哪些条件" 
docs = rules_db.similarity_search(question,k=5)
for doc in docs:
    print(doc)
    
# 对不同的embeddding模型进行比较，data数据不变，关键是生成chroma的时间和搜索的质量
# bert-base-chinese
# 效果改善很多
# hfl/chinese-bert-wwm
# 结果是一样的
# hfl/roberta-base-chinese
# 报错没找到
# hfl/chinese-roberta-wwm-ext
# 与bert的结果略有不同
embedding_text = "hfl/chinese-roberta-wwm-ext"
chunk_size = 50
chunk_overlap = 5
embeddings = SentenceTransformerEmbeddings(model_name=embedding_text)
text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap,length_function=len,add_start_index=True)
data = text_splitter.split_documents(documents=pdf)
# 使用多个分词器运行多次的结果是有多条记录 
rules_db = chroma.Chroma.from_documents(data, embeddings)  
question = "客户开展融资融券业务应该具备哪些条件" 
docs = rules_db.similarity_search(question,k=5)
for doc in docs:
    print(doc)
# 打印元数据
# metadata是一个字段
print(docs[2].metadata['source'])
temp = docs[2].metadata
temp['source']
print(temp)

#查询满足一定条件元数据
#如果遇到两个条件要用$and字符，不能直接使用字典
'''
metadata_query = {
    "source": docs[2].metadata['source'],
    "page": 8,

}
'''
metadata_query = {
    "$and": [
        {"source": docs[2].metadata['source']},
        {"page": 8},
        {"start_index": 561}
    ]
}
# 查询块开始点在100到200之间的
metadata_query = {
    "$and": [
        {"source": "D:\\script\\python\\data\\\\data\\margin_base.pdf"},
        {"page": 8},
        {"start_index": {"$gte": 100, "$lte": 200}}  # 查询 start_index 在 100 到 200 之间的分块
    ]
}


# 报错：AttributeError: 'Chroma' object has no attribute 'query'
# 使用get
# rules_db.query(where = metadata_query)
temp_source = rules_db.get(where = metadata_query)
temp_documents = temp_source['documents'] 

# 研究一下data的结构,开始的数字没有什么规律
data[10]
for i in range(50,100):
    print(data[i].metadata['start_index'])
# 其中有多少重复的，没有重复的，kimi给的解答可能是真的，实际上就是有三条记录，
import pandas as pd
data_pd = pd.DataFrame()
for i in range(len(data)):
    data_pd.loc[i,'source'] = data[i].metadata['source']
    data_pd.loc[i,'page'] = data[i].metadata['page']
    data_pd.loc[i,'start'] = data[i].metadata['start_index']

# 找出其中重复的数据,结果中没有重复的数字  
duplicates = data_pd[data_pd.duplicated()]

# 查看数据库基本信息,给出的数据是59805恰好是data数据信息的3倍
rules_db._collection.count()

# -*- coding: utf-8 -*-
"""
Created on Wed Jul 10 10:28:48 2024

@author: csc05176
"""

# 调用已经存在的数据库
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.vectorstores import chroma
persist_directory = r'C:\Users\csc05176\Desktop\py\deeplearning\data\db1'
embedding_text = "hfl/chinese-roberta-wwm-ext"
embeddings = SentenceTransformerEmbeddings(model_name=embedding_text)
vectordb = chroma.Chroma(persist_directory=persist_directory,embedding_function=embeddings)
# D:\tools\hf\.cache\huggingface\hub\models--hfl--chinese-roberta-wwm-ext 导入存量模型
# embedding_text = r"D:\tools\hf\.cache\huggingface\hub\models--hfl--chinese-roberta-wwm-ext\snapshots\5c58d0b8ec1d9014354d691c538661bf00bfdb44\\"
# embeddings1 = SentenceTransformerEmbeddings(model_name=embedding_text)
# No sentence-transformers model found with 标配即使你指定确定的有json文件的目录都会有
# 有指定json文件目录，不上网也可以加载，但是也会报not found
# 灭有指定目录，会报错连接超时，如果可以上网也不会真实下载，但是不报错可运行
# vectordb1 = chroma.Chroma(persist_directory=persist_directory,embedding_function=embeddings1)
question = "客户开展融资融券业务应该具备哪些条件" 
# 为了改善
# 搜索结果，做关键词方面的改进，但是不清楚机制，只能盲猜，不能推广
# 向量数据库的试用范围，同义词相关的，精确的只是多字段匹配感觉不适用向量数据库搜索

question1 = "融资融券业务具备的条件" 
question2 = "客户开展融资融券业务应该具备什么条件"
docs = vectordb.similarity_search(question1,k=5)
for doc in docs:
    print(doc)
# 获取metadata数据
page =[]
source = []
for doc in docs:
    page.append(doc.metadata['page'])
    source.append(doc.metadata['page'])
# 感觉没有500个子那么多，只有100个字，查询
# 数量少的原因是因为按照页进行分词，从第一个字开始，不跨页
metadata_query = {
    "$and":
    [{"source": 'C:\\Users\\csc05176\\Desktop\\py\\deeplearning\\data\\\\证券金融部制度汇总\\中信建投证券股份有限公司融资融券业务客户服务实施细则.pdf'},
    {"page": 2}
    #,{'start_index': 394}
    ]
}
temp_source = vectordb.get(where = metadata_query)
# 
len(temp_source['documents'][1])
# 遍历整个模型，遍历的结果不是pdf或是文档层面的相连
qurey = vectordb.get()['documents']
query_source = vectordb.get()['metadatas']
# 找到涉及内容的上下文
page = docs[1].metadata['page']
content =  []
for doc in docs:
    page = doc.metadata['page']
    source = doc.metadata['source']
    start = doc.metadata['start_index']
    page_list = []
    if page==0:
        page_list = [0,1]
    else:
        page_list = [page-1,page,page+1]
    for page in page_list:
        metadata_query = {
            "$and":
            [{"source": doc.metadata['source']},
            {"page": page}
            ]
            }
        temp_source = vectordb.get(where = metadata_query)
        long_string = " ".join(temp_source['documents'])
        content.append(long_string)
            
    

